<doc>
  <source auto="true" type="str" verify="true"><![CDATA[https://cyberleninka.ru/article/n/klassifikatsiya-tekstov-na-osnove-otsenki-semanticheskoy-blizosti-terminov]]></source>
  <category auto="true" type="str" verify="true"><![CDATA[Автоматика]]></category>
  <author auto="true" type="list" verify="true">
    <item type="str"><![CDATA[Нгуен Ба Нгок]]></item>
    <item type="str"><![CDATA[Тузовский Анатолий Федорович]]></item>
  </author>
  <title auto="true" type="str" verify="true"><![CDATA[Классификация текстов на основе оценки семантической близости терминов]]></title>
  <keywords auto="true" type="list" verify="true">
    <item type="str"><![CDATA[КЛАССИФИКАЦИЯ ТЕКСТОВ]]></item>
    <item type="str"><![CDATA[СЕМАНТИЧЕСКАЯ БЛИЗОСТЬ]]></item>
    <item type="str"><![CDATA[МАТРИЦА СОВМЕСТНОЙ ВСТРЕЧАЕМОСТИ]]></item>
    <item type="str"><![CDATA[АННОТАЦИЯ ПО ЧАСТЯМ РЕЧИ]]></item>
  </keywords>
  <annotation auto="true" type="str" verify="true"><![CDATA[Рассматривается способ увеличения точности классификации текстов по алгоритму kNN путем применения оценки семантической близости на основе матрицы совместной встречаемости терминов. Предлагается метод уменьшения размера матрицы совместной встречаемости путем фильтрации терминов по частям речи. Выполнена проверка влияния метода фильтрации на точность классификации.The article considers the method for increasing text classification accuracy by the kNN algorithm applying the estimation of semantic similarity based on terms co-occurrence matrix. The authors propose the method for decreasing the size of co-occurrence matrix filtering terms by pats of speech. The influence of filtration technique on classification accuracy is tested.]]></annotation>
  <text auto="true" type="str" verify="true"><![CDATA[Нгуен Ба Нгок, А.Ф. Тузовский Томский политехнический университет E-mail: nguyen_bn@hotmail.com Рассматривается способ увеличения точности классификации текстов по алгоритму kNN путем применения оценки семантической близости на основе матрицы совместной встречаемости терминов. Предлагается метод уменьшения размера матрицы совместной встречаемости путем фильтрации терминов по частям речи. Выполнена проверка влияния метода фильтрации на точность классификации. Ключевые слова: Классификация текстов, kNN, семантическая близость, матрица совместной встречаемости, аннотация по частям речи. Key words: Text classification, kNN, semantic similarity, co-occurrence matrix, parts of speech annotation. Введение Классификация текстов по тематическим категориям является актуальной задачей, которая требуется в различных информационных системах. Общая задача классификации данных рассматривается в книге [1]. В данной статье под термином классификатор понимается алгоритм решения задачи классификации текстов по тематическим категориям. Среди существующих классификаторов, таких, как Naive Bayes [1, 2], Rocchio [1, 3], SVM[1, 4], kNN [1] и т. д., самым быстрым и интуитивно понятным является классификатор kNN (k-Nearest Neighbor -ближайший из k соседей). По алгоритму kNN результирующим тематическим классом классификатора является класс, имеющий максимальную оценку близости kNN (D) = Kmax, при этом Kmax определяется по условию: sim( D, Kmax) = max( sim( D, Ki)), i=1—n где sim(D,K) - оценка близости документа D корпусу Ki. Целью данной работы является повышение точности работы алгоритма kNN с помощью применения оценки семантической близости на основе матрицы совместной встречаемости терминов в тексте. Предложенный алгоритм сравнивается по точности с классическим текстовым классификатором kNN. Постановка задачи Заданы k тематических классов, при этом, каждый i-й тематический класс определяется коллекцией (корпусом) документов Ki, которая состоится из множества текстовых документов по i-й теме. Каждая коллекция Ki также называется обучающей выборкой i-го тематического класса: K,. = {Di, A,-, Dn.}, где каждый документ Dj состоится из множества терминов: Dj = ft, t2,-, tmj }. Для любого текстового документа D, который не входит в обучающую выборку Ki, требуется определить наиболее подходящий тематический класс. 1. Классификатор kNN в векторном пространстве Классический алгоритм kNN для классификации текстов базируется на оценки близости в векторном пространстве. Для этого выполняется представление документа в виде вектора терминов. 1.1. Представление документов в векторном пространстве Пусть T - множество всех уникальных терминов системы: T = ft, t2,-, tn}, которое определяет «-мерное пространство, где каждый термин ti соответствует одной размерности пространства. В данном «-мерном пространстве каждый документ D коллекции K соответствует точке, которая определяется вектором vD - представление документа D: Vd = ((ti, ©1 ),(t2, Ю2), —,(tn , 0n ) ), где Wi - весовой коэффициент термина t. Одним из наиболее эффективных методов вычисления весовых коэффициентов является метод tf.idf, по которому значение wt определяется следующим образом: f Wd,k (ti),если ti G D; 0, в противном случае. 1.2. Схема вычисления весовых коэффициентов tf.idf Весовой коэффициент термина по схеме tf.idf вычисляется как произведение нормализованной частоты встречаемости термина f (term frequency) и обратного значения нормализованной частоты документов idf (inverted document frequency). Коэффициент і/термина і документа Б определяется по следующей формуле: /гед_п(£) tfD(t) = : lDl где fredD(t) - частота встречаемости термина t в документе D; |D\ - количество всех терминов документа D (размер документа D). Нормализованная частота документов df (document frequency) термина t в корпусе K определяется по формуле: dfK (t) = \{ЩРІ є K. t є D}| |K| где |К - количество документов коллекции К; обратное значение нормализованной частоты документов вычисляется следующим образом: ( i idfK (t) = log I dfK (t). Весовой коэффициент а і термина tt документа D в корпусе K вычисляется по следующей формуле: r .jr , s freqD(t) С 1K ^ tf • idfD,K(t) = —iTj-----log lDl |K| |{DJt є Dt. D, є K}| Ck = у ЬєК D |k| ((ti. ci). (t2. c2). " " ", (4 . cn ) ). где ув - вектор представления документа Б; іі - термины; с! - весовой коэффициент, который определяется следующим образом: c= а, К где aiD - весовой коэффициент термина t\ документа D. 1.4. Классический алгоритм kNN для классификации текстов Оценка близости классифицируемого документа D и обучающей выборки K определяется как близость их векторов представления в векторном пространстве sim(D. К) = simvector (VD. CK ). На основе данной оценки близости результат классификатора kNN определяется следующим образом: kNN (D) = Кmax. при этом Kmax определяется по условию: sim. vector (VD . CKm„ ) = (simvector (VD . CK ))s 1.3. Обучение классификатора Обучение классификатора кЫЫ заключается в вычислении центральных векторов обучающих выборок. Под центральным вектором СК обучающей выборки К понимается следующий вектор: где К - множество обучающих выборок; 1.5. Оценки близости векторов Оценка близости векторов а и Ь в векторном пространстве обозначается 5/т?ес1о1(а,Ь). Методы вычисления близости векторов представляются в таблице. 2. Классификатор kNNс использованием оценки семантической близости По предлагаемой схеме для вычисления оценки семантической близости, сначала требуется определение матрицы совместной встречаемости терминов. При этом отличаются два типа оценки семантической близости: 1) терминов; 2) коллекций. 2.1. Матрица совместной встречаемости терминов Матрица совместной встречаемости ИК коллекции К определяется в множестве уникальных терминов Т следующим образом: Мк 0'> ]) = Мк 0; > tj ) = /геЧк ({; > 0 )> где ^-еТ;/гед(1,,1) - частота совместной встречаемости, т. е. частота, по которой термин ^ встречается вместе с термином ^в коллекции К. Считается, что термин ^ встречается вместе с термином ^, если расстояние между ними в тексте не превышает предела х: 1 Таблица. Методы вычисления оценки близости между векторами Название метода Формула Inv. Sq. City-block- обратное значения расстояния по Сити-Блок метрики (city-block distance) в квадрате -ы°ск (a.b) = (у fli - ь |)2 +1 Inv. Sq. Euclidean - обратное значение евклидова расстояния в квадрате siminv.sq.euclidean (a,b) = ^ , ,,2 , У, (a- b) +1 Cosine - косинус угла между векторами У ab simcos,ne(a,b) = ‘ д/У a УЛ Correlation - корреляция между векторами ■ / ,ч Уі (ai - a)(bi - b) simcorrelation (a,b) = Г= =- VZ i (ai - a)2 Уі (bi - b)2 Имеется следующий псевдокод алгоритма определения матрицы совместной встречаемости MK коллекции документов K: MK=[0] // в начале алгоритма, все элементы матрицы равны нулю for DeK do for teD do for j=i —z to i+z do if i! = j then MK(tj,ti)++ // увеличение на единицу Главной проблемой создания матрицы совместной встречаемости является большой требуемый объем оперативной памяти для её сохранения. В следующем разделе представляется решение данной проблемы путем фильтрации терминов. 2.2. Метод фильтрации терминов Целью применения метода фильтрация терминов является уменьшение размера матрицы совместной встречаемости путем снижения количества терминов коллекции, при этом поддерживается минимальная потеря точности классификации. В данной статье предлагается метод фильтрации терминов фильтрации терминов по частям речи, согласно которому из коллекции документов удаляются все термины, относящиеся к части речи, которая является исключением. Для чего, сначала требуется выполнение аннотирования текстов по частям речи (parts ofspeech tagging [5]). Одним из эффективных инструментов для выполнения аннотирования текстов на английском языке является библиотека Stanford CoreNLP (http://nlp.stanford.edu/ software/corenlp.shtml), аналогом которой для текстов на русском языке является библиотека MyStem (http://company.yandex.ru/techno-logies/ mystem). 2.3. Оценка семантической близости терминов На основе матрицы совместной встречаемости терминов MK, значение термина tt определяется i-й строкой матрицы Мк следующим образом: meanK (ti ) = ((t1, W1), (t2, W2), —, (tn , Wn )}, где wj=MK(ti,tj) - частота совместной встречаемости терминов ti и tj. Семантическая близость терминов определяется как близость между их значениями, представленными в виде векторов. Семантическая близость термина ti коллекции K1 и термина tj коллекции K2 определяется следующим образом: simsem (tiKl , tjK2 ) = ^vector (meanK1 (ti ),mednK 2 (tj )). Другие методы определения семантической близости терминов на основе матрицы совместной встречаемости представляются в работах [6, 7, 8]. 2.4. Оценка семантической близости коллекции документов Оценка семантической близости коллекций документов K1 и K2 определяется как сумма оценки семантической близости терминов следующим образом: SІmsm(K1, K2 ) = S teTSimvector(meanK1(t ),mednK2(t )). Идентично, оценка семантической близости документа D и коллекции K вычисляется по следующей формуле: SІm3em(D, K ) = S leTSim vector (mean d (t ),meanK (t )). 2.5. Алгоритм кММс использованием оценки семантической близости С использованием оценки семантической близости предлагается следующая комбинация (combination) оценок близости документа D и коллекции K: Simcomb (A K) = Sim vector (VD , CK ) + simsern (D, K), результат классификатора kNN определяется следующим образом: kNN (D) = Kmax, при этом, Kmax определяется по условию: Simcomb (D, Kmax ) = ma^(SimCOmb (D, K)). KeKi 3. Эксперимент В качестве тестовых данных используется коллекция 20Newsgroups (http://people.csail.mit.edu/jren-nie/20Newsgroups), которая предназначена для тестирования метода классификации. Коллекция 20Newsgroups содержит 20 тысяч текстовых сообщений, которые разделяются на 20 тематических групп новостей. Каждая группа имеет приблизительно 1000 сообщений содержащихся 400 документов для тестирования и 600 документов для обучения классификатора. С использованием данной коллекции были выполнены три эксперимента для проверки точности предлагаемого метода классификации и размера матрицы совместной встречаемости. Оценка точности классификации в одной группе новостей определяется как отношение между количеством правильных результатов и общим количеством документов группы. Итоговая оценка точности классификации вычисляется как среднее значение оценок точности классификации в отдельных группах. В данной статье предлагается определение размера матрицы совместной встречаемости M как количество её ненулевых элементов: size(M) = |{(t,., tj) | tt, tj e T,M(ti,tj) > 0}|. 3.1. Методы вычисления близости векторов Значения точности предлагаемого алгоритма и классического алгоритма в зависимости от используемого метода вычисления близости векторов (см. таблица) и коэффициента z (см. раздел 2.1) представлены на рис. 1. Рис 1. Оценки точности классификации Горизонтальная ось представляет собой значения коэффициента z, а вертикальная - значения точности классификации. Четыре серии данных соответствуют значениям точности классификации при использовании разных оценок близости векторов simW(ior: cosine - косинус; correlation - значение корреляции; inv.sq.eucledian - обратное значение евклидова расстояния в квадрате; inv.sq.city-block - обратное значение расстояния по сити-блок метрики в квадрате. Исходя из полученных данных, для всех методов вычисления близости векторов можно отметить существенное увеличение точности классификации в результате использовании оценки семантической близости по сравнению с классическим методом, т. е. точность классификации в случаях z>0 значительно выше, чем точность классификации в случае z=0 (разница >7 %). Наивысшая точность для классического метода классификации («74 %) достигается при использовании метода вычисления близости векторов по косинусу (cosine), z=0. По сравнению с классическим методом наивысшая точность для предлагаемого метода классификация («82 %) достигается при использовании метода вычисления близости векторов по косинусу и z=4. Кроме того, если z>0, то идентичные результаты также получаются с применением значения корреляции между векторами. Самая низкая точность получается при использовании метода вычисления близости на основе сити-блок-метрики и метода вычисления близости на основе евклидова расстояния (<30 %). 3.2. Размер матрицы совместной встречаемости Зависимость размера матрицы совместной встречаемости от метода фильтрации терминов и значения расстояния z представлены на рис. 2. Горизонтальная ось представляет собой значения коэффициента z, а вертикальная ось - значения размера матрицы совместной встречаемости. Три серии данных соответствуют значениям размера матрицы, полученным при использовании разных методов фильтрации терминов, в каждых сохраняются: All - все термины; Noun & Verb - только существительные и глаголы; Noun - только существительные. Исходя из полученных данных, видно, что в результате фильтрации терминов значительно уменьшается размер матрицы совместной встречаемости по сравнению с случаем использования всех терминов: приблизительно 30 % в случае использовании только существительных и глаголов и приблизительно 50 % в случае использования только существительных. Кроме того, также видно, что с условием фиксированного метода фильтрации, если увеличивается расстояние z, то резко увеличивается размер матрицы совместной встречаемости (рис. 2), однако точность классификации увеличивается незначительно (рис. 3). 3.3. Точность классификатора с использованием метода фильтрации терминов Зависимость точности классификатора от используемого метода фильтрации терминов и значения расстояния z представлено на рис. 3. В каче- 12 х 1 ю ® 8 м я (S О- ° 4 2 и 1 2 3 4 5 ■ АЛ 2,89 5,31 7,53 9,59 11,54 ■ Noun & Verb 1,96 3,59 5,11 6,54 7,89 ■ Noun 1,56 2,83 4,01 5,12 6,17 Рис 2. Размер матрицы совместной встречаемости стве оценки близости векторов используется оценка близости по косинуса, так как, исходя из первого эксперимента, она является лучшим методом определения близости векторов. Горизонтальная ось представляет собой значения коэффициента z, а вертикальная ось - значения точности классификации. Три серии данных соответствуют значениям точности, полученным при использовании разных методов фильтрации терминов: All - используются все терминов; Noun & Verb - используются только существительные и глаголы; Noun - используются только существительные. | 80 X т о н Класстескийр ^ метод 4— - Предлагаемый м— метод 0 1 1 2 3 4 5 —♦—АД 73,72 | 81,00 81,65 82,06 82,21 82,19 Noun & Verb 75,01 ; 16,12 77,00 76,95 77,13 77,42 A Noun 75,56 ! 16,15 77,23 77,12 77,12 77,16 Рис. 3. Влияние метода фильтрации на точность классификатора Исходя из полученных данных видно, что в случае г=0 (классический алгоритм кЫЫ), максимальная точность (75,56 %) получается при сохранения в коллекции документов только существительных. Однако в остальных случаях z>0 (используется оценка семантической близости), наивысшая точность («82 %) получается при использовании всех терминов, и если применяется метод фильтрация, то снижается точность классификации. Кроме того, в случае использования метода фильтрации терминов при увеличении г значение точности увеличивается незначительно в пределах 3...4 %, по сравнению с 7 % в случае без использования метода фильтрации (ЛИ). Выводы Показано, что: 1) лучшим методом оценки векторной близости (среди рассмотренных) является оценка близости по косинусу; 2) метод фильтрации существительных имеет большое практическое значение для классического алгоритма кЫЫ в векторном пространстве, так как, при сохранении только существительных из коллекции документов, точность классификации является максималь- ной, одновременно количество используемых терминов для классификации значительно сокращается; 3) с использованием оценки семантической близости точность классификатора кЫЫ значительно увеличивается по сравнению с классической реализацией (>7 %), однако требуется дополнительные затраты для вычисления матрицы совместной встречаемости и оценки семантической близости. ]]></text>
</doc>
