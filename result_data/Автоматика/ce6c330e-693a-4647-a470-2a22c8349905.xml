<doc>
  <source auto="true" type="str" verify="true"><![CDATA[https://cyberleninka.ru/article/n/algoritm-obnaruzheniya-faktov-dublirovaniya-informatsii-v-dokumentirovannyh-rezultatah-samostoyatelnoy-uchebnoy-deyatelnosti]]></source>
  <category auto="true" type="str" verify="true"><![CDATA[Автоматика]]></category>
  <author auto="true" type="list" verify="true">
    <item type="str"><![CDATA[Маликов Андрей Валерьевич]]></item>
    <item type="str"><![CDATA[Целиковский Алексей Сергеевич]]></item>
  </author>
  <title auto="true" type="str" verify="true"><![CDATA[Алгоритм обнаружения фактов дублирования информации в документированных результатах самостоятельной учебной деятельности студентов, устойчивый к незначительным изменениям текста]]></title>
  <keywords auto="true" type="list" verify="true">
    <item type="str"><![CDATA[определение плагиата]]></item>
    <item type="str"><![CDATA[метод шинглов]]></item>
    <item type="str"><![CDATA[синонимайзеры]]></item>
    <item type="str"><![CDATA[нормализация слов]]></item>
    <item type="str"><![CDATA[definition of plagiarism]]></item>
    <item type="str"><![CDATA[method of shingles]]></item>
    <item type="str"><![CDATA[synonymy software]]></item>
    <item type="str"><![CDATA[normalization of words]]></item>
  </keywords>
  <annotation auto="true" type="str" verify="true"><![CDATA[Проблема дублирования информации в работах студентов очевидна. Существующие методы определения плагиата требуют полного соответствия записей отдельных частей текста для установления факта дублирования, поэтому они не могут корректно обрабатывать результаты обработки текстов программными синонимайзерами, которые в последнее время получили довольно широкое распространение. В статье описывается модификация метода шинглов для определения плагиата, позволяющая определять заимствования в текстах после обработки их синонимайзерами посредством морфологической и синонимической нормализации слов.]]></annotation>
  <text auto="true" type="str" verify="true"><![CDATA[© 2011 г. А.В. Маликов, А.С. Целиковский Северо-Кавказский государственный технический университет, г. Ставрополь North Caucasus State Technical University, Stavropol Проблема дублирования информации в работах студентов очевидна. Существующие методы определения плагиата требуют полного соответствия записей отдельных частей текста для установления факта дублирования, поэтому они не могут корректно обрабатывать результаты обработки текстов программными синонимайзерами, которые в последнее время получили довольно широкое распространение. В статье описывается модификация метода шинглов для определения плагиата, позволяющая определять заимствования в текстах после обработки их синонимайзерами посредством морфологической и синонимической нормализации слов. Ключевые слова: определение плагиата; метод шинглов; синонимайзеры; нормализация слов. The problem of duplication in the work of student is evident. Existing methods for determining plagiarism cannot correctly handle the results of word processing synonymy software, which recently received fairly widespread. The article describes a modification of the definition of plagiarism shingles that enables the borrowing of texts after their treatment by synonymy software Keywords: definition of plagiarism; method of shingles; synonymy software; normalization of words. В условиях беспрепятственного доступа студентов к огромным информационным ресурсам (интернет, электронные библиотеки, работы других студентов, в том числе параллельных и предыдущих курсов) существует большое искушение при выполнении домашних заданий (рефераты, курсовые проекты и т.д.) использовать отдельные части уже существующих документов. В итоге часто работы получаются собранными из отдельных «лоскутов». В связи с развитием различных языковых утилит (в частности, синонимай-зеров) задача распознавания дублирования становится еще сложнее. Цель данной работы состоит в усовершенствовании существующих алгоритмов поиска дубликатов для повышения полноты результатов. Предыдущие работы Поиск полных дубликатов: для каждого документа вычисляется хэш-функция. Выявление дубликатов производится путем поиска значения данной функции в имеющейся базе. Обычно используются MD2, MD5, или SHA алгоритмы хэш-функций. Необходимые свойства этих функций: широкая область определения на различных данных различной длины, высокая скорость, низкая степень хэш-коллизий. Проблемы данного подхода очевидны: хэш-функции обычно очень неустойчивы к незначительным изменениям файлов - форматирование текста, изменение заголовка, опечатки и т.д. В связи с этим применяют алгоритмы нечеткого сравнения. В каждом методе на предварительном этапе из документов удаляются разметка, стоп-слова и иные термы, не подходящие по статистическим характеристикам. Векторная модель информационного поиска. В рамках этой модели каждому терму в документах сопоставляется некоторый неотрицательный вес. Таким образом, каждый документ может быть представлен в виде ¿-мерного вектора. Сравнение производится путем вычисления косинуса угла между векторами. Если значение будет выше некоторого порога, документы считаются дубликатами. Данный метод не учитывает структуры документов и вычисляет лишь сходство двух документов, не учитывая их длины. Сложность алгоритма O(n2), где n - количество сравниваемых документов. Метод шинглов (DSC) [1]. Вместо отдельных термов в данном методе используется хэш-функции их последовательностей определенной длины, называемые шинглами. Получаем множество шинглов для каждого документа. Из множества хеш-кодов последовательностей, в соответствии с некоторой схемой случайного отбора, выбирается подмножество, которое и служит так называемым «отпечатком» (образом) документа. На основе этих шинглов можно также использовать векторную модель либо меру Жакара для определения сходства: АП в\ Sim(A, B) = . |AU B| Сложность алгоритма O(n2) - при сравнении с каждым документом коллекции, где n - количество документов. При использовании инвертированного индекса по шинглам сложность определяется скоростью работы поиска по индексу. Метод супершинглов (DS^SS). Подобен простому методу шинглов но вместо последовательностей термов использует последовательность шинглов, таким образом, уменьшая количество шинглов и соответственно сравнений. Метод лексического типа (I-Match) [2] использует свою базу лексикона, на основе которой строится образ документа. В образ входят лишь те термы, которые содержатся в лексиконе. Из образа строится хэш-код документа. Метод не устойчив к изменениям текста, поэтому помимо стандартной сигнатуры, создается еще несколько, каждая из которых получается случайным удалением некоторой доли всех термов из исходной сигнатуры. Документы считаются очень сходными, если их наборы сигнатур имеют большое пересечение хотя бы по одной из сигнатур. Сложность O(n log n), где n - количество документов коллекции. Разрабатываемый алгоритм Рассмотренные методы, как видно, обладают рядом недостатков: неустойчивы к изменениям документов, не учитывают морфологические и синонимические вариации термов, составляющих образ документа. За основу был взят метод DSC. В нем были сделаны следующие усовершенствования: 1. Нормализация порядка, для повышения устойчивости алгоритма к изменению порядка слов в предложении. Заключается в сортировке слов в шингле. 2. Синонимическая нормализация, позволяющая бороться с автоматической заменой слов их синонимами (программными синонимайзерами). Состоит в замене каждого слова базовым синонимом, общим для всех синонимов данного слова. 3. Морфологическая нормализация, для повышения устойчивости алгоритма к изменениям морфологических форм слов. Заключается в приведении слов к начальной форме и удалении стоп-слов. Наибольший интерес представляет собой второй пункт изменений. Необходимо при сравнении документов учесть все синонимы каждого слова. При решении задачи в «лоб», т. е. при полном переборе всех синонимов при построении шингла, возникает комбинаторный взрыв, так как каждая последовательность слов в итоге представляется 2n шинглами, где n -общее количество синонимов слов в данной последовательности. Для решения задачи представим словарь синонимов в виде графа G := (V, E), где V - слова, а E - синонимичные связи между словами. Интуитивно возникает желание извлечь компоненты связности и выбрать в каждой компоненте базовый синоним и при обработке текста заменять слова базовыми синонимами компонент связности, которым они принадлежат. Однако оказалось, что из-за многозначности и непредсказуемости языка, размеры компонент связности могут состоять из тысячи слов. Выделение клик, также не принесло успеха, так как по той же причине огромное количество слов принадлежат огромному количеству клик. Было принято решение применить некую «кластеризацию» синонимов, позволяющую выделить отдельные области графа, сильно связанные внутри и слабосвязанные между собой. Для реализации этой идеи использовался алгоритм Штор-Вагнера [3], который позволяет найти минимальный разрез графа, т.е. разделить граф на 2 компоненты связности с удалением минимального количества ребер. В исходном алгоритме в качестве критерия удачного разделения используется суммарный вес ребер между компонентами. В нашем случае V К = 1-п"Ч-Г к , \Еси\(1 + К -У2\) где V - количество вершин в графе; к1,к2 - количество вершин в первом и втором подграфе соответственно; \ЕШ\ - количество ребер «между» подграфами; к - коэффициент. Вычисление оптимального коэффициента является отдельной задачей. Для наших экспериментов к = 1. Алгоритм был запущен рекурсивно для каждого найденного подграфа. Условием выхода из рекурсии являлось наличие условно максимального пути, не превышающего трех, в качестве которого мы взяли максимальный путь от произвольной вершины. Сложность алгоритма представляет собой: 0((пт + п2^(п))с), где п - количество вершин исходного графа; т - количество ребер исходного графа; с - количество кластеров. Так как кластеризация вычисляется один раз, данная сложность не является критичной. В результате «кластеризации» образовались области (кластеры), связанные между собой. В каждом кластере выберем его главный базовый синоним, а для каждой вершины, имеющей связь с вершинами, не принадлежащими кластеру (внешними вершинами), обозначим дополнительные базовые синонимы, являющиеся базовыми синонимами внешних вершин, с которыми данная вершина соединена. Например, на рисунке изображены отдельные кластеры. Крупными точками обозначены главные базовые синонимы кластеров. Тогда для В главным базовым синонимом будет F, дополнительным D; для F главным F, дополнительных нет; для С главным D, дополнительных нет; для D аналогично F; для А главным D, дополнительными F, Е и G. Далее работает следующий принцип. При индексации документов с целью построения шингла мы используем для каждого слова только его главный базовый синоним (который является единственным для каждого слова). При поиске документа помимо шингла с главными базовыми синонимами строим шинглы для всех дополнительных базовых синонимов. Таких шинглов получается опять же 2п шинглов на каждую последовательность слов, где п' - общее количество дополнительных базовых синонимов в этой последовательности слов. Но благодаря «кластеризации», дополнительных базовых синонимов на порядок меньше, чем прямых синонимов слова. Для уменьшения размерности задачи используются выкладки, рассмотренные в [1]. Сейчас же рассмотрим корректность алгоритма нормализации синонимов. Возвращаясь к рис. 1, рассмотрим несколько ситуаций, которыми можно описать все возможные. Кластеры будем обозначать именами главных базовых синонимов. 1. Очевидно, что если при индексации и поиске встречаются два слова из одного кластера, они дадут совпадения. 2. При индексации В (В ^ Е), при поиске С (или любая другая вершина, у которой нет связи с кластером К (С ^ {О}). Совпадения нет - корректно. 3. При индексации В (В ^ Е), при поиске А (А ^ {О, К, О, Е}). Совпадения есть - корректно, так как А связано с вершиной В. 4. Однако, при индексации С (С ^ , как и любая вершина из D), при поиске В (В ^ {Е, D}) - совпадение есть. Это небольшая погрешность в точности для поддержания межкластерной связи. Поступила в редакцию Сложность алгоритма не изменится, так как дополнительные операции, менее сложные (сортировка и поиск в инвертированной базе синонимов сводятся к ntlog(Nt), где п - количество термов в анализируемом документе, N - количество термов в базе данных), проходят в качестве подготовки к основному алгоритму. Вывод На базе метода шинглов [1] разработан новый алгоритм поиска дубликатов, устойчивый к незначительным изменениям текста, таким как: изменение порядка слов в предложении, изменение морфологических форм слова, а также замена слов их синонимами. Данный алгоритм позволяет определять факт дублирования текстов, после обработки их программными синонимайзерами. При проведении экспериментов использовался грамматический словарь компании Solarix. Работа выполнена в рамках реализации ФЦП «Научные и научно-педагогические кадры инновационной России» на 2009-2013 гг. по проблеме «Разработка теоретических основ функционирования систем управления полуструктурированными данными». ]]></text>
</doc>
