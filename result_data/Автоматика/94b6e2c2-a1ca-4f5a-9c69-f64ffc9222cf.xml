<doc>
  <source auto="true" type="str" verify="true"><![CDATA[https://cyberleninka.ru/article/n/obektnaya-model-jscala]]></source>
  <category auto="true" type="str" verify="true"><![CDATA[Автоматика]]></category>
  <author auto="true" type="list" verify="true">
    <item type="str"><![CDATA[Самоваров О.И.]]></item>
    <item type="str"><![CDATA[Арапов И.В.]]></item>
    <item type="str"><![CDATA[Бабкова В.В.]]></item>
  </author>
  <title auto="true" type="str" verify="true"><![CDATA[Объектная модель JSCALA]]></title>
  <keywords auto="true" type="list" verify="true"/>
  <annotation auto="true" type="str" verify="true"><![CDATA[В статье рассматривается объектная модель JSCALA, реализующая пакет прикладных программ SCALAPACK в среде ParJava. Эта модель позволяет создавать переносимые, масштабируемые, параллельные программы решения задач линейной алгебры в среде ParJava. На примере модели JSCALA показано, как следует включать в среду ParJava высокоуровневые модели параллельного программирования.]]></annotation>
  <text auto="true" type="str" verify="true"><![CDATA[О. И. Самоваров, И. В. Арапов, В. В. Бабкова Аннотация. В статье рассматривается объектная модель JSCALA, реализующая пакет прикладных программ SCALAPACK в среде ParJava. Эта модель позволяет создавать переносимые, масштабируемые, параллельные программы решения задач линейной алгебры в среде ParJava. На примере модели JSCALA показано, как следует включать в среду ParJava высокоуровневые модели параллельного программирования. 1. Введение. В последнее время широкое распространение получили параллельные архитектуры с распределенной памятью - кластерные системы. Одной из трудностей, препятствующих эффективному использованию таких систем, является отсутствие языковых средств высокого уровня, поддерживающих разработку эффективных параллельных программ. Сегодня существует достаточно много различных подходов к созданию параллельных программ, но наиболее популярным, ставшим фактически стандартом является интерфейс MPI (Massage Passing Interface) [1]. Модель данных, поддерживаемая MPI, предоставляет программисту средства, позволяющие организовать межпроцессорные взаимодействия. Обычно это примитивы, реализующие синхронные или асинхронные пересылки данных между процессами виртуальной сети. Пересылки могут быть организованы как между отдельной парой процессов, так и между всеми членами группы процессов, объединенных общим контекстом. Параллельная программа, использующая MPI, разрабатывается в терминах модели передачи сообщений. Эго значит, что программисту необходимо решить ряд задач, связанных с распределением данных и распределением вычислений, необходимо также организовать доступ, к удаленным данным, используя примитивы MPI. Однако существует ряд моделей параллелизма, позволяющих рассматривать параллельную программу в определениях той прикладной области, задачи которой она решает. Среди таких моделей можно выделить DVM [2], DPJ [3] и др. Эти модели позволяют разрабатывать параллельные программы в естественной для прикладной области системе обозначений. Например, для DVM, модели предназначенной для решения сеточных задач линейной алгебры - это распределенные массивы, параллельные циклы, решетки процессоров. Для DPJ, модели параллельной обработки объектов - это распределенные контейнеры, итераторы и параллельные алгоритмы. В ИСП РАН разрабатывается интегрированная система ParJava [4], поддерживающая разработку масштабируемых параллельных программ на языке Java [5] с использованием интерфейса MPI. Среда ParJava включает в себя графический редактор, отладчик, профилировщик, визуализатор трасс и другие средства отладки и анализа параллельных Java-nporpaMM. Также реализованы и интегрированы в среду ParJava модели высокого уровня DPJ и Java-DVM. Используя соответствующие библиотеки классов, программист имеет возможность разрабатывать параллельные программы, используя различные модели параллелизма. Дальнейшая разработка и реализация моделей высокого уровня параллелизма является одним из важных направлений развития среды ParJava. Наиболее широкое применение высокопроизводительные системы и параллельное программирование находят при решении вычислительных задач. Среди существующих средств разработки параллельных программ ориентированных на решение задач линейной алгебры можно выделить пакет прикладных программ SCALAPACK [6] созданный на факультете «Computer Science» Университета штата Теннеси под руководством Жака Донгарра. В статье рассматривается объектная модель JSCALA, которая представляет собой объектную реализацию пакета программ SCALAPACK и предназначена для решения задач линейной алгебры. При разработке модели JSCALA требовалось сохранить оригинальную архитектуру SCALAPACK, но в то же время использовать объектную концепцию проектирования, заложенную в Java. В разделе 2 представлен краткий обзор пакета SCALAPACK, рассматриваются его архитектура и функциональности. Раздел 3 посвящен описанию объектной реализации пакета SCALAPACK на языке Java - JSCALA. В заключении (раздел 4) делаются выводы относительно перспектив развития среды ParJava. 2. SCALAPACK - пакет программ для решения задач линейной алгебры. Аббревиатура SCALAPACK расшифровывается как «Scalable Linear Algebra Package», что можно перевести на русский язык как «пакет масштабируемых программ, предназначенный для решения задач линейной алгебры». Функционально пакет SCALAPACK разделен на несколько компонент: LAPACK (Linear Algebra PACKage) - набор подпрограмм, предназначенный для решения систем линейных уравнений, уравнений метода наименьших квадратов, задач на собственные значения, задач с сингулярными или плохо обусловленными матрицами. Высокая эффективность этих подпрограмм обусловлена тем, что в них используются алгоритмы, которые базируются на вызовах функций библиотеки BLAS. Кроме того, каждая подпрограмма пакета имеет один или несколько платформо-зависимых параметров, влияющих на эффективность ее выполнения. Эти параметры определяются в процессе инсталляции пакета и используются во время выполнения программы. □ BLAS (Basic Linear Algebra Subprograms) - содержит базовые функции линейной алгебры, такие как вычисление скалярного произведения, умножение матрицы на вектор, умножение матриц и т.п. Как известно эффективность выполнения операций с матрицами, в особенности умножение матриц, в большой степени зависит от особенностей архитектуры вычислительного комплекса. Функции BLAS настраиваются таким образом, чтобы учитывать эти особенности. BLAS можно рассматривать как уровень, обеспечивающий эффективность и переносимость SCALAPACK-алгоритмов. □ PBLAS (Parallel Basic Linear Algebra Subprograms) - библиотека функций, обеспечивающих параллельное выполнение базовых алгоритмов линейной алгебры. □ BLACS (Basic Linear Algebra Communication Subprograms) - библиотека подпрограмм межпроцессорного взаимодействия, ориентированная на решение задач линейной алгебры. Вычислительная модель этой библиотеки базируется на предположении о том, что части массивов и векторов распределяются на одно- или двумерную решетку процессов. Решетка процессов создается и может модифицироваться использованием функций BLACS. Кроме того, BLACS содержит функции, позволяющие организовать синхронные взаимодействия между процессами. В алгоритмах пакета SCALAPACK часто требуется организовать широковещательный обмен данными или выполнить редукционную операцию на части процессах решетки. В BLACS решетка процессов может быть разделена на подмножества, присвоением каждому процессу своего контекста. Основной задачей BLACS можно считать обеспечение переносимости коммуникационного уровня пакета SCALAPACK. На рис. 1. представлена иерархия программных модулей пакета SCALAPACK. Компоненты (программные модули), расположенные в области «Local», реализуют последовательные алгоритмы и используют только локальные данные. Модули, расположенные в области «Global», реализуют параллельные алгоритмы решения задач линейной алгебры. Для их выполнения данные (матрицы и векторы) должны быть предварительно распределены на процессоры вычислительной сети. ScaLAPACK Рис. 1. Иерархия программных модулей пакета SCALAPACK. 3. Объектная модель JSCALA. Существует два способа переноса SCALAPACK на Java. Первый заключается в создании Java-интерфейса для пакета SCALAPACK и реализации этого интерфейса, используя вызовы подпрограмм, входящих в состав SCALAPACK. Второй способ предполагает проектирование новой объектной модели, но в этом случае возникнет необходимость реализации всех подпрограмм SCALAPACK в виде Java-ioaccoB и их методов. В данной статье рассматривается второй способ. Объектная модель JSCALA структурно повторяет SCALAPACK и также разделена на уровни, как показано на рис. 1. Методы решения задач линейной алгебры собраны в двух классах: □ JSCALADriverRoutines - класс «ведущих» методов, каждый из которых решает законченную задачу, например решение системы линейных уравнений или вычисление собственных значений симметричных матриц. Более подробно методы этого класса описаны в разделе 3.4.1. □ JSCALAComputationalRoutines - класс «вычислительных» методов, предназначенных для решения специальных задач, таких как LU-факторизация, приведение вещественных, симметричных матриц к трехдиагональному виду и т.п. Необходимо отметить, что методы этого класса используются в тех случаях, когда пользователю для решения его задачи не достаточно стандартных «ведущих» методов, он может сконструировать свой собственный «ведущий» метод, используя «вычислительные» методы. В разделе 3.4.2. содержится подробное описание методов этого класса. Оба класса наследуются от класса JSCALA, содержащего общие поля и методы, такие как: □ jbcomm — объект класса JLAComm, реализует коммуникационный уровень модели и содержит методы межпроцессорного взаимодействия ориентированные на решение задач линейной алгебры. □ jbpla - объект класса JPBaseLA, реализует базовые методы линейной алгебры. Описанные выше классы являются отображением уровня SCALAPACK в иерархии программных модулей (см. рис. 1). Коммуникационный уровень модели JSCALA поддерживается классами JLACommPVM и JLACommMPI. В первом случае в качестве базовой среды межпроцессорного взаимодействия используется пакет PVM (Parallel Virtual Machine) [6], во втором - MPI. Оба класса реализуют интерфейс JLACommI описывающий методы коммуникационной подсистемы специализированной на решение задач линейной алгебры. Пользователь имеет возможность, реализовав этот интерфейс самостоятельно, создать собственную версию коммуникационного уровня рассчитанного на иную базовую коммуникационную подсистему. Классы JLACommPVM и JLACommMPI являются отображением BLACS в иерархии программных модулей SCALAPACK(cm. рис. 1). Кроме того, необходимо заметить, что коммуникационный уровень JSCALA привязан к классам ParJava реализующим примитивы MPI и PVM. Важной частью модели являются классы JPBaseLALevell, JPBaseLALevel2, JPBaseLALevel3, реализующие базовые методы линейной алгебры. Методы этих классов поддерживают параллельной выполнение алгоритмов и в качестве параметров используют распределенные данные. Подробное описание классов базовой линейной алгебры можно найти в разделе 3.3. Данный уровень объектной модели является отображением BLAS и PBLAS в иерархии программных модулей SCALAPACK (см. рис. 1). Распределенные данные - матрицы и векторы, объектной модели JSCALA реализуются целым набором классов. Поскольку в Java нет удовлетворительной поддержки многомерных массивов, предлагается использовать специальные классы, которые позволяют создавать двухмерные матрицы различных типов: IntegerArray, FloatArray, DoubleArray. Реализовав интерфейс Arrayl, пользователь может создать матрицу собственного типа данных. Для того чтобы определить распределенную матрицу или вектор, пользователь должен создать экземпляр класса IntegerDArray, FloatDArray, DoubleDArray, передав в качестве параметра ссылку на соответствующий скалярный объект. Класс Descriptor содержит описание способа распределения матрицы, размер распределенной части матрицы, контекст и другую информацию необходимую, для обеспечения доступа к элементам распределенных данных. На рис. 3. представлена диаграмма классов модели JSCALA. 3.1. Распределенные данные. Модель JSCALA предполагает, что данные (векторы и матрицы) должны быть распределены на процессоры. Распределенные данные в JSCALA представляются специальными классами IntegerDArray, FloatDArray, DoubleDArray. Используя эти классы, пользователь может создать распределенный массив или вектор нужного типа данных. Эти классы реализуют интерфейс Arrayl, используя который пользователь может создать распределенный массив или вектор специального типа. Общие данные отображаются на локальную память узлов в соответствии с определенным способом распределения. В JSCALA существует три вида распределения данных: блочное, циклическое и блочно циклическое. Блочное распределения данных предполагает, что матрица или вектор делятся пропорционально количеству процессоров, образуя равные блоки которые хранятся в локальной памяти. Циклическое распределение описывает тот случай, когда элементы матрицы распределяются циклически на узлы, образуя смешенные блоки данных каждый из которых хранится на локальном узле. Блочно-циклическое распределение представляет собой совокупность первого и второго способа. В этом случае пользователь может задать размер блоков матрицы, которые будут распределяться циклически на узлы. 3.2. Коммуникационный уровень модели JSCALA. Коммуникационный уровень модели JSCALA представлен классами JLAComm, JLACommPVM, JLACommMPI, а также интерфейсом JLAComml. Этот уровень модели реализует систему межпроцессорных обменов, необходимую для выполнения параллельных алгоритмов линейной алгебры. Кроме того, интерфейсы коммуникационного уровня позволяют использовать эффективные, низкоуровневые средства межпроцессорных обменов и обеспечивают переносимость программ модели JSCALA между широким спектром вычислительных платформ с распределенной памятью. Система межпроцессорных обменов модели JSCALA основа на следующих ключевых идеях: Стандартный интерфейс. Одной из основных идей коммуникационного уровня JSCALA является то что, классы и методы, используемые для обеспечения межпроцессорных обменов, могут быть использованы на любой из поддерживаемых платформ. Интерфейсом между платформами в описываемой модели является СГЬАСотт1. Текущая версия ^САЬА содержат два класса СГЬАСоттРУМ и СГЬАСоттМР1, реализующих интерфейс JLACornmI. Эти классы обеспечивают взаимодействие с двумя широко распространенными коммуникационными подсистемами - РУМ и МР1. Решетка процессов. Вычислительная модель ^САЬА базируется на предположении о том, что части массивов и векторов распределяются на одно или 2-х мерную решетку процессов. Решетку процессов в данном случае можно представить как абстрактный параллельный компьютер процессоры которого пронумерованы как 0,1,...,Р-1. Тогда решетка процессоров будет иметь РТ строк процессоров и Рс столбцов процессоров, где РТ*РС = Р ■ Каждый процессор может быть проиндексирован координатами РТ,РС, где 0 <рт <РТ и 0 <рс <РС . Пример такого представления процессов приведен на рис. 2, где Рт = 2 и Рс = 4 . D 1 2 3 0 1 2 3 4 Б б 7 Рис. 2. Восемь процессов, представленных в виде решетки размерностью 2x4. На рис. 2 процессы отображаются на решетку процессоров, используя строковый порядок, иначе говоря, нумерация процессов в решетке производится по строкам слева направо. Существует еще один порядок нумерации процессоров в решетке -нумерация сверху вниз, такой способ отображения называется «по столбцам». Метод gridint () класса JLAComm реализует отображение процессов на решетку процессов. По умолчанию используется «строковый» метод отображения. Метод gridmap класса JLAComm является общим и позволяет пользователю более гибко задавать отображения процессов. Области операций обменов. В JSCALA поддерживаются так называемые «области» операций обменов. Это значит, что система, использующая линейный массив процессов, имеет область действия операции обмена, равную всем процессам. Однако, если используется двухмерная решетка процессов, существует три области действия операций обмена: Row - все процессы в строке; Column - все процессы в столбце; АН - все процессы в решетке. о Arrayl IntegerArray JLACommPVM ♦getlxO 'getlyO ^swapRowO ' ^swapColumns() ^transposeArrayO igetWidth() г ♦getHeightO -IgetXOO I ♦getYOQ Float Array Array DoubleArray ч ^>'xD E'yD Qjwidth ^height JLAComm <%my_ _3_num : Integen 0n_procs: Integer ^>i_contxt: Integer ^np_ row: Integer ^>n_col: Integer .row: Integer Bimv col: Integer d|>p_row: Integer ^p_col: Integer : Integer JLACommMPI ^>type ^ctxt ^jrow Sbcol ^>rowb ^pcolb ^rSrc ^>cSrc *%lld 3getType() PfgetCtxtQ □IgetRowO □feetColO P^getRowbQ ESgetColbO PjgetRsrcQ CSgetCsrcO □IgetLldQ JSeaLA ..1 1.. ■ JPBaseLA ^jbeomm : JLAComrrv Hjbpla : JPBaseLA ^jcommla: JCommLA О JLAComml ^setupO ♦getO ♦setO HgridlnitO ■plnfoO P^gredMapQ <>freeBuffO 3gredExitO ^abortO ^exitO HforedlnfoQ HpNumO KibarrierO BipCoordO ■gSnd2dO %Snd2dO P^gRcv2dQ HtRcv2dO HgbSnd2dO BtbSnd2dO r%3bRcv2dQ HtbRcv2d() HgSum2dO ^gMax2dO BSgMin2d() JScaLAComputationalRoutines ^psdbtrfO IpsdbtrsO |psdbtrsv() ^psdttrfO ^psdttrsO ^psdttrsvQ HpsgbtrfO HpsgbtrsO HpsaebrdO Hpsaeconfi HpsgeequO J§psgehrd() ^psgelqfO HpsgeqlfO BpsgeqpfO ■psgeqrfO ^psgerfsO J§psgerqf() JScaLADriverRoutines JPBaseLALevel2 IpsdbsvO ■psdtsvO Hpsgbsv() ^>psgelsO jfpsgesvQ J§psgesvd() Bpspbsvfi JlpsposvO HpsptsvO %>ssyev() ^pssyevdQ ^psgesvxO ^psposvxO ^pssyevxQ %)ssygvxQ JPBaseLALeveH О JBasePLALevelll KiswapO HscalO ■copy() ■axpy() ■dotO Bdotu() GdotcO Binrm20 K|asum() Ha max 0 Q JBasePLALevel3l JBasePLALevel2l ^gemmO ^symmO 4' HfaemvQ %iemmO ^hemvO |syrk() BsymvO ^herk() HtrmvO ♦syr2k() ^trsvO ♦her2kQ 'gero ♦geruO ♦gercO ^herO ■her2() *syrO ■syr2() Puc. 3. Диаграмма классов объектной модели JSCALA. Контекст. В модели ^САЬА каждая решетка процессов имеет свой контекст. Также контекст ассоциируется с каждой распределенной матрицей. Использование контекста обеспечивает возможность разделить пространство обмена сообщениями. Эго означает, что решетка процессоров обеспечивает безопасные коммуникации даже в то время, когда другая решетка процессоров так же производит операции с 94 обменом сообщениями. Похожая концепция разделения коммуникационных пространств используется и в MPI. Используемая в JSCALA концепция контекстов позволяет следующее: □ Создавать случайные группы процессов □ Создавать неопределенное число перекрывающихся процессорных решеток □ Разделять решетку процессов так чтобы, обмены производимые между узлами решетки не пересекались. Класс JLAComm модели JSCALA имеет два метода gridinit() и gridmap(), позволяющие создавать решетки процессоров в собственном контексте. Эти методы возвращают контекст, который представляет собой простое целое число, определяемое внутри класса. Контекст является расходуемым ресурсом, поэтому он должен быть освобояеден, после того как в нем пропала необходимость. Освобождение ресурса осуществляется вызовом метода gridexit(). 3.3. Классы базовых методов линейной алгебры. Модель JSCALA содержит классы, описывающие функции линейной алгебры. Такие как скалярное произведение, умножение матрицы и вектора, умножение матриц. Методы этих классов обеспечивают параллельное выполнения базовых алгоритмов линейной алгебры. Классы этого уровня модели разделены на три типа. Первый обеспечивает операции вида вектор-вектор, второй реализует операции вектор-матрица и третий поддерживает операции матрица-матрица. Ниже перечислены методы классов всех трех типов реализованных в данной версии JSCALA. 3.3.1. Базовые методы первого уровня класса JPBaseLALevell. Методы данного класса реализуют базовые параллельные алгоритмы линейной алгебры первого уровня: □ void swap(a,b) - поменять два распределенных вектора, □ double SCAl(a,c) — умножить элементы вещественного, распределенного вектора а на скаляр с, □ void copy(a,b) - скопировать один распределенный вектор в другой, □ void axpy(a,b) - добавить один распределенный вектор к другому, □ double dot(a,b) - внутреннее произведение двух распределенных векторов □ double nrm2(a) — вычислить вторую норму распределенного вектора □ double asum(a) - вернуть сумму абсолютных значений распределенного вектора □ double amax(a) - найти максимальный элемент распределенного вектора а и его индекс 3.3.2. Базовые методы второго уровня класса JPBaseLALevel2. Методы данного класса реализуют базовые параллельные алгоритмы линейной алгебры второго уровня: □ double gemv(a,b,A,X) - выполнить операцию: Y = аА*Х + bY, где а и b - скаляры, X и Y - распределенные векторы, А - распределенная матрица. □ double hemv(a,b,A,X) - выполнить операцию: Y = аА*Х + bY, где а и b — скаляры, X и Y — распределенные векторы, А — распределенная Эрмитова матрица. □ double symv(a,b,A,X) - выполнить операцию: Y = аА*Х + bY, где а и b — скаляры, X и Y — распределенные векторы, А — распределенная симметричная матрица. □ double trmv(A) - выполнить операцию: Х = А*Х где X — распределенный вектор, А — распределенная верхняя или нижняя треугольная матрица. □ double trsv(A, X) - решить систему уравнений: А*Х=Ь, где b и X — распределенные векторы, А — распределенная верхняя или нижняя треугольная матрица. □ double ger(a, X,Y) - выполнить операцию: А = aX*Y+A, где а - скаляр, X и Y - распределенные векторы, А - распределенная матрица. □ double her(a,X) — выполнить Эрмитову операцию первого порядка: А = a*X*conjg(X)+A, где а — вещественный скаляр, X — распределенный вектор, А — распределенная Эрмитова матрица. □ double her2(a,A,X,Y) - выполнить Эрмитову операцию второго порядка: А = a*X*conjg(Y)+conjq(a)*Y*conjg(X)+A где а — вещественный скаляр, X и Y — распределенные векторы, А — распределенная Эрмитова матрица. □ double syr(a,X) — выполнить симметричную операцию первого порядка: А = а*Х*Х’+А, где а — вещественный скаляр, X — распределенный вектор, А — распределенная симметричная матрица. □ double syr2(a,X,Y) — выполнить симметричную операцию второго порядка: А = a*X*Y' + aY*X'+A, где а — вещественный скаляр, X и Y — распределенные векторы, А — распределенная симметричная матрица. 3.3.3. Базовые методы третьего уровня класса JPBaseLALevel3. Методы данного класса реализуют следующие базовые параллельные алгоритмы линейной алгебры: □ double gemm(a,b,A,B) - выполнить операцию: С=а*ор(А)*ор(В)+Ь*С, где а и Ь — скаляры, А, В, С — распределенные матрицы. □ double symm(a,b,A,B) - выполнить операцию: С=а*А*В+Ь*С, где а и b скаляры, А — симметричная, распределенная матрица, В, С — распределенные матрицы. □ double hemm(a,b,A,B) - выполнить операцию: С=а*А*В+Ь*С, где а и b скаляры, А - Эрмитова, распределенная матрица, В, С - распределенные матрицы. □ double syrk(a,b, А) — выполнить симметричную операцию порядка к: С=а*А*А’+Ь*С, где а и b — скаляры, С — симметричная, распределенная матрица, А -распределенная матрица. □ double herk(a,b,A) — выполнить Эрмитову операцию порядка к: С=а*А*А’+Ь*С, где а и b скаляры, С - Эрмитова распределенная матрица, А - распределенная матрица. □ double syr2k(a,b,A,B) — выполнить симметричную операцию порядка 2к: С=а*А*В’+аВ*А’+Ь*С, где а и b скаляры, С — симметричная распределенная матрица, А и В — распределенные матрицы. □ double her2k(a,b,A,B) - выполнить Эрмитову операцию порядка 2к: C=a*A*conjg(B’)+conjg(a)*B*conjg(A’)+b*C, где а и b скаляры, С - Эрмитова распределенная матрица, А и В - распределенные матрицы. 3.4. Классы, реализующие параллельные алгоритмы линейной алгебры. В модели JSCALA описываются два класса методов реализующих алгоритмы, предназначенные для решения задач линейной алгебры. Первый -JSCALAD rive г Routines, - содержит методы решения законченных задач, таких как решение системы линейных уравнений, вычисление собственных значений симметричных матриц и др. Этот класс базируется на методы класса JSCALAComputationalRoutines. В классе собраны методы, предназначенные для выполнения специальных операций, например LU факторизация, приведение вещественных, симметричных матриц к трехдиагональному виду и пр. Рассмотрим описываемые классы JSCALAD rive г Routines и JSCALAComputationalRoutines более подробно. 3.4.1. Класс «ведущих» методов JSCALADriverRoutines . Здесь и далее методы этого класса мы будем называть «ведущими». Методы класса JSCALADriverRoutines решают общие задачи линейной алгебры: Решение систем линейных уравнений. В классе реализовано два метода решения систем линейных уравнений. Простой метод psgesv () - решает систему АХ=В факторизацией А и переписыванием В с решением X. И расширенный метод psgesvx(), который выполняет некоторые или все из следующих функций: Т Н решение А X = В или А X = В (за исключением случаев, когда А симметричная или Эрмитова матрица); улучшение решения и вычисление передних и задних границ ошибок. Вычисление собственных значений. Решение задачи нахождения собственных значений сводится к нахождению характеристических чисел Я и соответствующих собственных векторов z != О, таких что Az = Яг , А = АТ № где А — вещественная матрица. Для Эрмитовых матриц мы имеем: Аг = Л,А = Ат В обоих случаях Я это вещественные значения. Когда будут найдены все характеристические числа и векторы, можно записать А = гкгт (4) где Л — диагональная матрица элементы, которой характеристические числа, а 2 — ортогональная матрица, столбцы которой - характеристические векторы. В классе азсАЬАБгл^егКоиЛл-пез есть метод pssyev(), который предназначен для вычисления всех характеристических чисел и векторов семеричной или Эрмитовой матрицы. Сингулярное разложение. Такое разложение матрицы размерностью т на п можно представить выражением: А = ШУТ А = ШУТ (®) где и и V ортогональны, 2 - это вещественная диагональная матрица размерностью ш на п элементы, которой вещественные числа. При этом должно выполняться условие: СГ1 — сг2 • • • ^"гшг^п^я) — 0 (7) ст 1 это сингулярные значения матрица А, а первая колонка пип(т,п) матриц и и V -это левый и правый сингулярные векторы матрицы А. Метод psgesvd() реализует разложение по сингулярным числам общих не симметричных матриц. Обобщенная симметричная задача собственных значений. Класс содержит метод который обеспечивает вычисление всех собственных значений и характеристических векторов следующих типов задач: Аг = ЛВг (8) АВг = Лг (9) ВАг = Лг (10) где А и В это симметричная или Эрмитова матрица и В положительно определена. Для всех задач характеристические значения Л вещественные. Когда матрицы А и В семеричные матрица 7. вычисленных собственных значений удовлетворяет условию: гТАг = к (11> в случае задач (8) и (10) и условию 1~ХА1~Т =1 (12> в случае задачи (9), где Л это диагональная матрица собственных значений на диагонали. 7. также удовлетворяет условию: 1ТВ1=1 (13> в случае задач (8) и (10) и условию гтв~1г = 1 (14> в случае задачи (9). Когда А и В Эрмитовы, матрица вычисленных собственных значений 7 удовлетворяет условиям: 1НА1=К (15> в случае задач (8) и (10) и условию 1~ХА1~Н =1 (16> в случае задачи (9), где Л это диагональная матрица собственных значений на диагонали. 7. также удовлетворяет условию: 1НВ1=1 (17> в случае задач (8) и (10) и условию 1НВ~Х1=1 (18> в случае задачи (9). 3.4.2. Класс «вычислительных» методов ^САЬАСошрг^а'Ыопатои'Ыпез. Методы этого класса мы будем называть «вычислительными», поскольку здесь решаются общие задачи линейной алгебры. Необходимо заметить, что с использованием этих методов реализован класс азсАЬАБгл^егКоиЛл-пез. Линейные уравнения. Систему связанных линейных уравнений можно записать как: Ах = Ь (19) где А это матрица коэффициентов, Ь это правая часть, а X решения. В (19) А определено как квадратная матрица порядка п, однако есть методы, в которых А может быть прямоугольной матрицей. Ортогональная факторизация и линейные задачи наименьших квадратов. ^САЬА предлагает методы факторизации прямоугольной матрицы А общего вида размерностью т на п, а также ортогональных матриц и треугольных матриц. С?/? факторизация. Наиболее общий и широко известный способ факторизации -это ()Р1 факторизация представленная выражением А = ^ т>п, где Р это треугольная матрица размерностью п на п и О это ортогональная матрица размерностью т на т. Метод класса рздедг£ () реализует вычисления ОЯ? факторизации. LQ факторизация. LQ факторизация определяется выражением: ( qA А = (L 0)Q = (L 0) =LQl if m<n, 102 ) где 1_ это треугольная матрица размерностью т на т, О это ортогональная матрица размерностью п на п. ()1 содержит первые т строк матрицы О, 02 содержит остальные п-т строки. Этот вид факторизации поддерживается методом рзде1д£(). Другие факторизации. ОЬ и ЯО факторизации можно записать как: Эти типы факторизаций вычисляются методами рздед1е () и рздег д£ (). Эти виды факторизаций не так часто используются как рассмотренные выше С®, и ЬО но могут оказаться полезными при разработке приложений, например для вычисления С®, факторизации. В текущей версии ^САЬА реализована лишь часть методов предлагаемых моделью 8САЬАРАСК. Полная версия этих классов - задача дальнейшего развития системы. 4. Заключение. В данной статье была рассмотрена объектная модель параллельного программирования ^САЬА. Модель ^САЬА расширяет среду Рап1а\а -интегрированную среду, предназначенную для разработки высокоэффективных параллельных программ. Приводится описание архитектуры модели, описывается диаграмма классов модели. Объектные модели параллелизма представляют собой технологию, способную обеспечить все этапы разработки эффективных параллельных программ: проектирование, реализацию и отладку. Объектные модели можно рассматривать как средства высокого уровня, которые способны дать ряд преимуществ при разработке параллельных программ. ]]></text>
</doc>
