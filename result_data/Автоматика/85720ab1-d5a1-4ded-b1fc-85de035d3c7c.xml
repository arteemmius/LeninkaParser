<doc>
  <source auto="true" type="str" verify="true"><![CDATA[https://cyberleninka.ru/article/n/modeli-poiska-struktur-dannyh-na-osnove-konkurentsii-i-kooperatsii]]></source>
  <category auto="true" type="str" verify="true"><![CDATA[Автоматика]]></category>
  <author auto="true" type="list" verify="true">
    <item type="str"><![CDATA[Виноградов Геннадий Павлович]]></item>
    <item type="str"><![CDATA[Мальков Александр Анатольевич]]></item>
  </author>
  <title auto="true" type="str" verify="true"><![CDATA[Модели поиска структур данных на основе конкуренции и кооперации]]></title>
  <keywords auto="true" type="list" verify="true">
    <item type="str"><![CDATA[КЛАСТЕР]]></item>
    <item type="str"><![CDATA[НЕЙРОНЫ]]></item>
    <item type="str"><![CDATA[САМООРГАНИЗАЦИЯ]]></item>
    <item type="str"><![CDATA[НЕЙРОННЫЕ СЕТИ]]></item>
  </keywords>
  <annotation auto="true" type="str" verify="true"><![CDATA[Рассматриваются и анализируются методы поиска кластеров, основанные на конкуренции и кооперации нейронов.]]></annotation>
  <text auto="true" type="str" verify="true"><![CDATA[1 2 Виноградов Г. П. , Мальков А. А. (Тверской государственный технический университет, Тверь) Рассматриваются и анализируются методы поиска кластеров, основанные на конкуренции и кооперации нейронов. Ключевые слова: кластер, нейроны, самоорганизация, нейронные сети. 1. Введение В процессе самоорганизации нейронных сетей в соответствии с поданными входными сигналами осуществляется активизация нейронов так, что в результате изменения их синаптических весов происходит адаптация к поступающим обучающим выборкам. Возникает своего рода положительная обратная связь: для тех нейронов, синаптические веса которых в большей степени соответствуют входным сигналам, их синаптические веса еще более возрастают. В результате происходит естественное расслоение нейронов на отдельные группы [1]. Отдельные нейроны или их группы сотрудничают между собой и активизируются в ответ на возбуждение, подаваемое конкретной обучающей выборкой. Одновременно происходит подавление других нейронов. В результате наблюдается конкуренция между 1 Геннадий Павлович Виноградов, кандидат технических наук, профессор 2 Александр Анатольевич Мальков, доцент, (kja227@list.ru) отдельными нейронами или группами и сотрудничество их внутри группы. Конкуренция наблюдается также и между нейронами внутри группы [2]. Среди механизмов самоорганизации можно выделить два основных класса: самоорганизация, основанная на ассоциативном правиле Хебба, и механизм конкуренции между нейронами на базе обобщенного правила Кохонена. Однако для всех способов обучения сети необходима избыточность обучающих данных, в противном случае обучение просто невозможно. 2. Теоретический анализ Пусть имеется непрерывное пространство входных сигналов (воздействий), которое обозначим через X. Это пространство характеризуется определенными метрическими связями векторов х е X. Обозначим через А дискретное выходное пространство. Его топология определяется упорядоченным множеством нейронов вычислительных узлов решетки. Обозначим через Ф нелинейное преобразование (которое называется картой признаков), выполняющее отображение входного пространства X в выходное пространство А: (1) Ф: X® А. Алгоритм формирования самоорганизующейся сети начинается с инициализации синаптических весов сети. Обычно это происходит с помощью назначения синаптическим весам малых значений, сформированных генератором случайных чисел. Это позволяет обеспечить отсутствие какого-либо порядка признаков. После корректной инициализации сети для формирования карты самоорганизации запускаются три основных процесса: конкуренция, кооперация и синаптическая адаптация. 3. Процесс конкуренции Пусть т - размерность входного пространства, входной вектор выбирается из входного пространства случайно и обозначается так: (2) X = [Х1, Хт]Т . Вектор синаптических весов каждого из нейронов сети имеет ту же размерность, что и входное пространство. Обозначим синаптический вес нейрона j следующим образом: (3) W] = W]m]T, j = 1, ..., Ь, где Ь - общее количество нейронов в сети. Для того чтобы подобрать наилучший вектор w^■, соответствующий входному вектору х, необходимо сравнить скалярные произведения wjíх, j = 1, ..., Ь и выбрать наибольшее значение. При этом предполагается, что ко всем нейронам применяется некоторое значение насыщения. Таким образом, выбирая нейрон с наибольшим значением произведения WjTх, определяется нейрон, местоположение которого должно стать центром топологической окрестности возбужденного нейрона. Отметим, что наилучший критерий соответствия, основанный на максимизации скалярного произведения WjTх, математически эквивалентен минимизации Евклидова расстояния между векторами х и w^■. Если использовать индекс ^х) для идентификации этого нейрона, который лучше всего соответствует входному сигналу х, то эту величину можно определить с помощью следующего соотношения (4) і(х) = аг§шіп х - н., у = 1, Ь . І 7 Нейрон с номером і(х), удовлетворяющий условию (4) называется нейроном-победителем для данного входного вектора х. За счет процесса конкуренции между нейронами непрерывное входное пространство входных сигналов активизации нейронов отображается в дискретное выходное пространство. 4. Процесс кооперации Нейрон-победитель находится в центре топологической окрестности сотрудничающих нейронов. Будем считать, что нейрон-победитель всегда пытается возбудить пространство близких к нему нейронов. Причем топологическая окрестность победившего нейрона j плавно сходит на нет с увеличением расстояния. Обозначим через И,- топологическую окрестность с центром в победившем нейроне 7, состоящую из множества возбуждаемых (кооперирующихся) нейронов, типичный представитель которого имеет индекс j. Пусть - латеральное расстояние между победившим (7) и вторично возбужденным (/) нейронами. Тогда можно предположить, что топологическая окрестность Н,/ является унимодальной функцией латерального расстояния йу и представляется в виде функции Гаусса: (5) А = ехр ( ¿2. ^ І,г 2о 2 Параметр а называется эффективной шириной топологической окрестности. Этот параметр определяет уровень, до которого нейрон из топологической окрестности нейрона победителя участвует в процессе обучения. Хотя можно рассматривать и другие формы топологической окрестности, но гауссова топологическая окрестность является наиболее подходящей с биологической точки зрения, чем, например, прямоугольная. Для обеспечения кооперации между соседними нейронами необходимо, чтобы топологическая окрестность И,- была зависимой от латерального расстояния между нейроном-победителем (7) и возбужденным нейроном (/) в выходном пространстве, а не от какой-либо меры длины в исходном входном пространстве, что и отражено в (5). В случае одномерной решетки расстояние й,- является целым числом, равным модулю \г -/\. В случае двумерной решетки это расстояние определяется выражением а2. = ]>. г . - г. ] . где дискретный вектор Г] определяет позицию возбуждаемого нейрона ], а г7 - победившего нейрона 7. Оба этих измерения проводятся в дискретном выходном пространстве. Для карт самоорганизации одним из условий является уменьшение с течением времени размера топологической окрестности. Это требование удовлетворяется за счет постепенного уменьшения ширины о функции топологической окрестности к,]. Популярным вариантом зависимости величины о от дискретного времени п является экспоненциальное убывание: Г \ П п = 0, 1, 2, (7) о (п) = о0 ехр где с0 - начальное значение величины о; т1 - заданная временная константа. Отсюда топологическая окрестность будет иметь форму, зависящую от времени, т. е.: Ґ (8) И (п) = ехр а 2. ],г 2о 2 (п) \ п = 0, 1, 2, где о(п) определяется по формуле (7). Таким образом, при увеличении количества итераций п ширина о(п) экспоненциально убывает, а вместе с ней соответственно сжимается и топологическая окрестность. Будем называть к7](п) функцией окрестности. 5. Процесс адаптации Для того чтобы сеть могла саморганизовываться, вектор синаптических весов М] нейрона ] должен изменяться в соответствии с входным вектором х. В постулате Хебба синаптический вес усиливается при одновременном возникновении предсинап-тической и постсинаптической активности. Но в случае обуче- 25 ния без учителя изменение в связях происходят только в одном направлении, что, в конечном счете, приводит все веса к точке насыщения. Для того чтобы обойти эту проблему, немного изменим гипотезу Хебба и введем в нее слагаемое забывания ё(у]№], где - вектор синаптических весов нейрона ]; g(y]) - некоторая положительная скалярная функция отклика У]. Единственным требованием, налагаемым на функцию g(y^■) является равенство нулю постоянного слагаемого разложения в ряд Тейлора функции g(y^■), что в свою очередь влечет выполнение соотношения: (9) g(У^) = 0 при У] = 0. Имея такую функцию, изменение вектора весов нейрона ] в решетке можно выразить следующим образом: (10) = ПУ]Х - g(Уj)Wj, где п - параметр скорости обучения алгоритма. Первое слагаемое в правой части (10) является слагаемым Хебба, а второе -слагаемым забывания. Для того чтобы выполнялось условие (9), выберем следующую линейную функцию g(Уj): (11) g(Уj) = ПУ]. Выражение (10) можно упростить, если принять, что (12) У] = ]■(*). Тогда подставляя (11-12) в (10) получим: (13) = фр1(х)(х - м>) Учитывая формализацию дискретного времени, для данного вектора синаптических весов м>](п) в момент времени п обновленный вектор М!](п + 1) в момент времени п + 1 можно определить в следующем виде: (14) (п +1) = (п) + п(п)](х)(п)х - (п) Это выражение применяется ко всем нейронам решетки, которые лежат в топологической окрестности победившего нейрона 7. Выражение (14) имеет эффект перемещения вектора синаптических весов Wj победившего нейрона 7 в сторону входного вектора х. При периодическом представлении данных обучения благодаря коррекции в окрестности победившего нейрона векторы синаптических весов будут стремиться следовать распределению входных векторов. Таким образом, представленный алгоритм ведет к топологическому упорядочиванию пространства признаков в выходном пространстве так, что нейроны, корректируемые в решетке, будут стремиться иметь одинаковые синаптические веса. Выражение (14) является формулой вычисления синаптических весов карты признаков. В дополнении к этому уравнению для выбора функции окрестности И]:7(х)(п) требуется учитывать эвристику (8). Параметр п(п) - скорость обучения также должен зависеть от времени, как это должно быть при стохастической аппроксимации. К примеру, можно начать с некоторого исходного значения г, а затем с течением времени постепенно уменьшать его. Это требование можно выполнить, выбрав для параметра интенсивности обучения следующую экспоненциальную функцию: где т2 - временная константа алгоритма самоорганизации. Формулы экспоненциального убывания параметров скорости обучения (15) и ширины функции окрестности (8) могут не являться оптимальными. Они просто адекватны процессу самоорганизующегося формирования карты признаков. 6. Описание алгоритма Суть алгоритма самоорганизации состоит в простом геометрическом вычислении свойств Хеббоподобного правила обучения и латеральных взаимодействий. Существенными характеристиками этого алгоритма являются следующие. Непрерывное входное пространство образов активации, которое генерируется в соответствии с некоторым распределением вероятности. Ґ Л п = 0, 1, 2, ..., Топология сети в форме решетки, состоящей из нейронов. Она определяет дискретное выходное пространство. Зависящая от времени функция окрестности hj:i(x)(n), которая определена в окрестности нейрона-победителя i(x). Параметр скорости обучения п(п), для которого задается начальное значение ho и который постепенно убывает во времени п, но никогда не достигает нуля. Для функции окрестности и параметра скорости обучения на этапе упорядочения (т. е. приблизительно для первой тысячи итераций) можно использовать формулы (8) и (14) соответственно. Для хорошей статистической точности на этапе сходимости параметр ц(п) должен быть установлен в очень малое значение (0,01 или меньше). Функция окрестности в начале этапа сходимости должна содержать только ближайших соседей нейрона-победителя. Алгоритм проходит четыре основных шага: инициализация, подвыборка, поиск максимального соответствия и корректировка. Кратко весь алгоритм можно описать следующим образом: Инициализация. Для исходных векторов семантических весов w;(0) выбираются случайные значения. Единственным требованием здесь является различие векторов для разных значений j = 1, ..., L, где L - общее количество нейронов в решетке. Подвыборка. Выбираем вектор x из входного пространства с определенной вероятностью. Размерность вектора равна т. Поиск максимального подобия. Находим наиболее подходящий (победивший) нейрон i(x) на шаге п, используя критерий минимума Евклидова расстояния: (16) i(x) = arg min j Коррекция. Корректируем векторы семантических весов всех активных нейронов, используя формулу: х - w . J . J = 1, .... L . (17) wj (п +1 = wj (п) + ц{пХ)Й х - wj (п) где п(п) - параметр скорости сходимости; ^^(п) - функция окрестности в победившем нейроне ?(х). Оба этих параметра динамически изменяются: ( \ п (18) п(п ) = П0ехр (19) Н (п) = ехр п = 0 , 1, 2, к; (20) о(п) = ^о ехр А ' _____]_± 2о 2 (п) V ( \ п п = 0, 1, 2, к: 0 т , п = 0, 1, 2, к 10 во время обучения с целью получения лучшего результата. Продолжение. Возвращаемся к шагу 2 и продолжаем вычисления до тех пор, пока в карте признаков не перестанут происходить заметные изменения. 7. Экспериментальная часть Работа описанного выше алгоритма проверялась на ряде тестовых примеров. В таблице 1 приведены центры и разбросы для распределения точек для примеров 1, 2. Координаты генерировались случайным образом. Таблица 1 Координаты центров Разбросы точек Пример 1 Пример 2 X У X У X У 0,3 0,6 ±0,15 ±0,15 ±0,5 ±0,2 0,3 0 ±0,15 ±0,4 ±0,15 ±0,4 0,3 -0,6 ±0,15 ±0,15 ±0,5 ±0,2 -0,3 0 ±0,15 ±0,15 ±0,4 ±0,3 На рис. 1 изображены результаты работы алгоритма конкуренции на первом тестовом примере. Квадратами указаны «истинные» центры, а ромбами - центры, найденные алгоритмом конкуренции. Крестики указывают первоначальное расположение весов нейронов. Координаты найденных центров приведены в таблице 2. Таблица 2 Таблица 2 Координаты найденных центров Пример Пример 2 0,3100 0,6100 -0,0830 -0,0166 0,3355 0,1080 0,3681 -0,5791 0,3075 -0,5841 -0,1213 -0,6094 -0,2953 0,0180 0,2450 0,5249 -0,4352 0,0061 -0,2127 0,6182 Уточнение координат центров Алгоритм конкуренции 1 -II---------,--------,-------,-------- -1 -0.5 0 0.5 1 Ох Рис. 1. Определение центров для тестового примера 1 Результат сравнивался с результатом работы алгоритма нечеткой самоорганизации. Результаты оказались практически одинаковы - разница получилась около 1%. На рис. 2 изображены результаты работы алгоритма конкуренции на втором тестовом примере. Здесь «истинные» центры не были смещены по отношению к примеру 1, а разбросы точек были увеличены. Из рис. 2 видно, что алгоритм конкуренции нашел 6 центров кластеризации. Найденные центры расположены таким образом, что они «притягивают» к себе точки разных частей одних и тех же сгущений точек. Кроме того, из рис. 2 видно, что 3 из найденных центров «описывают» «вертикальный» кластер, притягивая к себе определенную часть точек этого кластера. На этом же примере алгоритм нечеткой самоорганизации определил большее количество кластеров, один из которых оказался ложным. 1 Уточнение координат центров Алгоріггм конкуренции X X "-1 -0.5 0 0.5 1 Ох Рис. 2. Определение центров для тестового примера 2 Таким образом, алгоритм конкуренции показал эффективность, особенно в ситуации, когда кластеры имеют общие точки. ]]></text>
</doc>
